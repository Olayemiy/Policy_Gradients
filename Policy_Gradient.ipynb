{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy_Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ciAvre3l1HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd        \n",
        "from torch import Tensor                  \n",
        "import torch.nn as nn                     \n",
        "import torch.nn.functional as F           \n",
        "import torch.optim as optim               \n",
        "from torch.distributions import categorical\n",
        "import gym\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYanXJTbl4mP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "07508d0c-28ec-48e7-a4b8-ef1e7170bbb5"
      },
      "source": [
        "'''\n",
        "#if training with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_folder=\"/content/gdrive/My Drive/Policy_Gradient_save_files/\"\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#if training with google colab\\nfrom google.colab import drive\\ndrive.mount(\\'/content/gdrive\\')\\ndrive_folder=\"/content/gdrive/My Drive/Policy_Gradient_save_files/\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt73VW6tl75j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PGN(nn.Module):\n",
        "  def __init__(self, obs_size, n_actions):\n",
        "    super(PGN, self).__init__()               #size matters. bigger size == longer training time 128 neurons seemed just fine for this test\n",
        "    self.fc1= nn.Linear(obs_size, 128)\n",
        "    self.fc3= nn.Linear(128, n_actions)\n",
        "    \n",
        "  def forward(self, obs):\n",
        "    x= F.relu(self.fc1(obs))\n",
        "    x=self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc7GiMOQl8u1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  #hyperparameters\n",
        "  LR=0.01\n",
        "  GAMMA =0.99\n",
        "\n",
        "  #make environment\n",
        "  env = gym.make('CartPole-v1')  \n",
        "  obs=env.reset()\n",
        "  done = False\n",
        "                                                  \n",
        "  net= PGN(len(obs),env.action_space.n)\n",
        "  optimizer=optim.Adam(net.parameters(), lr=LR)\n",
        "  \n",
        "  action_list=[]\n",
        "  reward_list=[]\n",
        "  \n",
        "  count = 0                    #count number of episodes\n",
        "  num_episodes =500            #max number of episodes\n",
        "  max_r=0                      #for visualization purposes\n",
        "  \n",
        "  while (True):\n",
        "\n",
        "    output = net(torch.from_numpy(obs).float())\n",
        "    action_prob = torch.distributions.Categorical(F.softmax(output))\n",
        "    action = action_prob.sample()\n",
        "    obs, reward, done, info = env.step(action.item()) \n",
        "\n",
        "    #env.render()\n",
        "\n",
        "    max_r += reward\n",
        "\n",
        "    action_list.append(action_prob.log_prob(action))\n",
        "    reward_list.append(reward)\n",
        "    \n",
        "    if done:\n",
        "      optimizer.zero_grad()\n",
        "      print(max_r)                #print episode reward at end of episode\n",
        "      max_r =0 \n",
        "\n",
        "      obs=env.reset()\n",
        "\n",
        "      \n",
        "      total_reward=0\n",
        "      reward_list_calc=[]\n",
        "      for i in reversed(reward_list):                       #discount rewards. to do this in linear time, we traverse backwards\n",
        "        total_reward *=GAMMA \n",
        "        total_reward += i\n",
        "        reward_list_calc.append(total_reward) \n",
        "      reward_list = list(reversed(reward_list_calc))\n",
        "      reward_list = np.asarray(reward_list)\n",
        "      \n",
        "   \n",
        "      mean = np.mean(reward_list)\n",
        "      std = np.std(reward_list) if np.std(reward_list) > 0 else 1\n",
        "      reward_list = (reward_list-mean)/std                        #apparently z-scores are used to normalize data. we want to reduce variance here\n",
        "      \n",
        "\n",
        "      reward_list = torch.tensor(reward_list)   #this doesn't require grad either\n",
        "      \n",
        "      loss = 0\n",
        "      for r, logprob in zip(reward_list, action_list):\n",
        "          loss += -r * logprob\n",
        "\n",
        "      '''           \n",
        "      #------------------------------------Don't do this again--------------------------------------------------------------------------#\n",
        "      #this failed along with the tensor creation for action_list above: action_list = torch.tensor(reward_list, requires_grad = True)\n",
        "      loss = -reward_list*action_list\n",
        "      loss = torch.sum(loss)\n",
        "      '''\n",
        "\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "\n",
        "      reward_list=[]\n",
        "      action_list=[]\n",
        "\n",
        "      count+=1     #number of episode\n",
        "      \n",
        "      if(count==num_episodes):\n",
        "        print(\"done\")\n",
        "        break\n",
        "      \n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gYviCihl_BG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}